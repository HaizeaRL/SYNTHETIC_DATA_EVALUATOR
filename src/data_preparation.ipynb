{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "'''# metadata \n",
    "print(diabetes_130_us_hospitals_for_years_1999_2008.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(diabetes_130_us_hospitals_for_years_1999_2008.variables) '''\n",
    "  \n",
    "# fetch dataset \n",
    "diabetes_130_us_hospitals_for_years_1999_2008 = fetch_ucirepo(id=296) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = diabetes_130_us_hospitals_for_years_1999_2008.data.features \n",
    "y = diabetes_130_us_hospitals_for_years_1999_2008.data.targets \n",
    "\n",
    "# create complete real_data\n",
    "diabetes = pd.DataFrame(X)\n",
    "diabetes[\"readmitted\"] = y\n",
    "\n",
    "# visualize data\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE & PREPROCESS DATA\n",
    "\n",
    "### Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions\n",
    "print(f\"Dimension: {diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect sensitive columns by intuition by their name\n",
    "print(f\"\\columns: {diabetes.columns}\\n\")\n",
    "\n",
    "# identify identity sensible data: \n",
    "sensitive_columns = ['race', 'gender', 'weight', 'admission_type_id','discharge_disposition_id','admission_source_id','payer_code', 'medical_specialty']\n",
    "print(f\"\\nSensitive columns: {sensitive_columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check columns values & distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE COLUMN: check columns values & distribution\n",
    "def visualize_columns_distributions(df):\n",
    "    for col in df.columns:\n",
    "        print(f\"\\n\\nColumn: {col}\")\n",
    "\n",
    "        # Combine proportions into a DataFrame for easy comparison\n",
    "        data =  pd.DataFrame({\n",
    "            'Values': df[col].value_counts(dropna=False),\n",
    "            'Proportions': df[col].value_counts(normalize = True, dropna=False)\n",
    "        }).fillna(0)\n",
    "\n",
    "        # print data\n",
    "        print(data)\n",
    "    \n",
    "# call to columns_distibution function\n",
    "visualize_columns_distributions(diabetes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is not balanced they are more caucasian (%74) than any other race.\n",
    "\n",
    "###  Generalize 'Nan' race values to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize Nan race as 'Other'\n",
    "import numpy as np\n",
    "diabetes.loc[diabetes[\"race\"].isin([np.nan ,'Other']), \"race\"] = \"Other\"\n",
    "\n",
    "# validate change\n",
    "diabetes.race.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hispanic and Asian can be generalized as other race but this way still be considerably imbalanced. \n",
    "\n",
    "###  Check 'Unknown/Invalid' gender values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Unknown/Invalid' data\n",
    "diabetes[diabetes['gender'] == 'Unknown/Invalid'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 3 registry, not possible to define gender, best option would be to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing  'Unknown/Invalid' gender data\n",
    "print(f\"Shape before drop: {diabetes.shape}\")\n",
    "diabetes = diabetes.drop(diabetes[diabetes[\"gender\"] == 'Unknown/Invalid'].index)\n",
    "\n",
    "# validating results (only 3 less)\n",
    "print(f\"Shape after drop: {diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check for 'null values' per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nulls per columns (percentage)\n",
    "diabetes.isna().sum() * 100 / len(diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight column has plenty null values (%96.858387), best option would be to remove.\n",
    "\n",
    "###  Drop \"weight\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove weight column \n",
    "print(f\"Columns before remove {len(diabetes.columns)}\")\n",
    "diabetes = diabetes.drop('weight', axis=1)\n",
    "print(f\"Columns after remove {len(diabetes.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check for 'variability'\n",
    "\n",
    "Columns that have only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_without_variability(df):    \n",
    "    \"\"\"\n",
    "    Function that is responsible to determine which columnns has no variability (those which has only 1 value).    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        list(): list of variables without variability.\n",
    "    \"\"\"    \n",
    "    sobran = []\n",
    "\n",
    "    cols = df.columns\n",
    "    for col in cols:\n",
    "        if len(df[col].unique()) < 2:\n",
    "            print(f\"Column: `{col}` unique values: {df[col].unique()}\")\n",
    "            sobran.append(col)\n",
    "\n",
    "    return sobran\n",
    "\n",
    "# obtain column list without variability\n",
    "cols_without_variability = columns_without_variability(diabetes)\n",
    "\n",
    "# print result\n",
    "print(f\"Invariant columns: {cols_without_variability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns without variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns\n",
    "print(f\"Columns without variability: {cols_without_variability}\")\n",
    "print(f\"Columns before remove {len(diabetes.columns)}\")\n",
    "diabetes = diabetes.drop(columns = cols_without_variability)\n",
    "print(f\"Columns after remove {len(diabetes.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect 'single value entries'\n",
    "\n",
    "Columns that present a singularization risk, columns that have more than one value but only a single instance of one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_single_value_entries(df):\n",
    "    rows_to_check = []  # List to hold the rows matching the criteria\n",
    "    cols_to_check = []\n",
    "    for col in df.columns:\n",
    "        # Get the value counts for the column\n",
    "        value_counts = df[col].value_counts()\n",
    "\n",
    "        # Check if exactly one value has a count of 1\n",
    "        if (value_counts == 1).sum() == 1:\n",
    "            # Get the value that appears exactly once\n",
    "            single_value = value_counts[value_counts == 1].index[0]\n",
    "            # add column name\n",
    "            cols_to_check.append(col)\n",
    "            \n",
    "            # Select rows where this single value appears\n",
    "            matching_rows = df[df[col] == single_value]\n",
    "            \n",
    "            # Append these rows to the list\n",
    "            rows_to_check.append(matching_rows)\n",
    "\n",
    "    # Concatenate all the matching rows into a single dataframe (if needed)\n",
    "    result_df = pd.concat(rows_to_check, ignore_index=True) if rows_to_check else pd.DataFrame()\n",
    "\n",
    "    return result_df,cols_to_check\n",
    "\n",
    "# determine single value entries\n",
    "matching_rows_df, cols_to_check = determine_single_value_entries(diabetes)\n",
    "\n",
    "# check data relevancy\n",
    "for col in cols_to_check:\n",
    "    value_counts = diabetes[col].value_counts(dropna = False)\n",
    "    print(f\"Single value entry in column {col} :  {value_counts[value_counts == 1].index[0]}\")\n",
    "    #print(f\"Distribution \\n{value_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With synthetic data anonymization, this singularization risk can be diminished, but since there are few records, they can be analyzed to determine if they can be removed\n",
    "\n",
    "#### Check 'single value entries'\n",
    "\n",
    "The columns `metformin-pioglitazone`, `glimepiride-pioglitazone`, and `acetohexamide` show a steady value. Do they correspond to the same individual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter from matching_rows_df those only they have 1 Steady instances\n",
    "steadys = [\"metformin-pioglitazone\",\"glimepiride-pioglitazone\",\"acetohexamide\"] \n",
    "val =\"Steady\"\n",
    "\n",
    "# Filter rows where exactly one of the specified columns has the value 'Steady'\n",
    "filtered_df = diabetes.loc[\n",
    "    (diabetes[steadys[0]] == val).astype(int) +\n",
    "    (diabetes[steadys[1]] == val).astype(int) +\n",
    "    (diabetes[steadys[2]] == val).astype(int) == 1\n",
    "]\n",
    "\n",
    "# show results\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect 'sensitive columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect sensitive columns by intuition by their name\n",
    "print(f\"\\columns: {diabetes.columns}\\n\")\n",
    "\n",
    "# identify identity sensible data: \n",
    "sensitive_columns = ['race', 'gender', 'age', 'admission_type_id','discharge_disposition_id','admission_source_id','payer_code', 'medical_specialty']\n",
    "print(f\"\\nSensitive columns: {sensitive_columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sensitive_columns for any reidentification risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df[sensitive_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check also this columns values variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in steadys:\n",
    "    print(f\"Column: {col} \\nunique values: {diabetes[col].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three steady cases clearly pose a reidentification risk due to their singularization. If they are removed, the metformin-pioglitazone, glimepiride-pioglitazone, and acetohexamide columns will lose their variability and should also be removed.\n",
    "\n",
    "Check the other `single value entries` to evaluate if they should be removed. \n",
    "\n",
    "Check 'admission_source_id' = 13 case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk of reindetifiction better to be removed.\n",
    "diabetes[diabetes['admission_source_id'] == 13] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check 'payer_code' : 'FR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk of reindetifiction better to be removed.\n",
    "diabetes[diabetes['payer_code'] == 'FR'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for 'chlorpropamide' : 'Down'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variability of the column\n",
    "diabetes['chlorpropamide'].value_counts() # if registry is removed chlorpropamide column not need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for 'tolazamide' : 'Up'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variability of the column\n",
    "diabetes['tolazamide'].value_counts() # if registry is removed tolazamide column not need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate columns shape to determine if `steady` columns need to be removed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove steady rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows and columns as they do not have variability after removing\n",
    "print(f\"Actual dimension: {diabetes.shape}\")\n",
    "print(f\"Removing rows: {filtered_df.index}\")\n",
    "diabetes = diabetes.drop(filtered_df.index)\n",
    "print(f\"After dimension: {diabetes.shape}\")\n",
    "print(\"Checking column variability\")\n",
    "cols_without_variability = columns_without_variability(diabetes)\n",
    "print(f\"Removing columns: {cols_without_variability}\")\n",
    "diabetes = diabetes.drop(columns = cols_without_variability)\n",
    "print(f\"Later dimension: {diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the other single value registries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove single 4 rows\n",
    "print(f\"Current dimensions: {diabetes.shape}\")\n",
    "\n",
    "# Calculate how many rows will be removed\n",
    "rows_to_remove = len(diabetes[(diabetes['chlorpropamide'] == 'Down') ^ \n",
    "                              (diabetes['tolazamide'] == 'Up') ^ \n",
    "                              (diabetes['admission_source_id'] == 13) ^ \n",
    "                              (diabetes['payer_code'] == 'FR')])\n",
    "print(f\"Removing {rows_to_remove} rows.\")\n",
    "\n",
    "# Drop the rows based on conditions\n",
    "diabetes = diabetes.drop(diabetes[(diabetes['chlorpropamide'] == 'Down') ^ \n",
    "                                  (diabetes['tolazamide'] == 'Up') ^ \n",
    "                                  (diabetes['admission_source_id'] == 13) ^ \n",
    "                                  (diabetes['payer_code'] == 'FR')].index)\n",
    "\n",
    "print(f\"Updated dimensions: {diabetes.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check dtype uniformity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "print(f\"\\nData information: {diabetes.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical data comprobations\n",
    "num_cols = diabetes.select_dtypes(include='int64')\n",
    "\n",
    "# Check column values, correspond to dtypes\n",
    "for cat in num_cols.columns:\n",
    "    print(f\"\\nColumn: {cat} values: {diabetes[cat].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change admission_type_id, discharge_disposition_id  &  admission_source_id  to categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# admission_type_id, discharge_disposition_id  &  admission_source_id  are categorical no numericals. Change\n",
    "cols_to_change = [\"admission_type_id\",\"discharge_disposition_id\", \"admission_source_id\"]\n",
    "diabetes[cols_to_change] =  diabetes[cols_to_change].astype(str)\n",
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get updated list\n",
    "num_cols = diabetes.select_dtypes(include='int64')\n",
    "\n",
    "# Check column values, correspond to dtypes\n",
    "for cat in num_cols.columns:\n",
    "    print(f\"\\nColumn: {cat} values: {diabetes[cat].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize memory use changing object to string\n",
    "categorical_cols = diabetes.select_dtypes('object').columns.tolist()\n",
    "\n",
    "# Check column values, correspond to dtypes\n",
    "for cat in categorical_cols:\n",
    "    print(f\"\\nColumn: {cat} values: {diabetes[cat].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check categorical column pair relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# get categorical column pairs\n",
    "categorical_col_pair = list(itertools.combinations(categorical_cols, 2))       \n",
    "\n",
    "# visualize data relations\n",
    "for pair in categorical_col_pair:\n",
    "    print(f\"\\n{pair[0]} distribution per {pair[1]}\")\n",
    "    print(f\"{diabetes.groupby(pair[0])[pair[1]].value_counts(dropna= False).unstack().fillna(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create folder\n",
    "tmp_folder = \"./tmp_folder\"\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "\n",
    "# save data\n",
    "diabetes.to_parquet(os.path.join(tmp_folder,\"refined_file.parquet\"),engine=\"pyarrow\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDV SYNTHETIC DATA GENERATION\n",
    "\n",
    "## LOAD PREPROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# create folder\n",
    "tmp_folder = \"./tmp_folder\"\n",
    "diabetes = pd.read_parquet(os.path.join(tmp_folder,\"generalized_file.parquet\"),engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE SYNTHETIZER & SYNTHETIC DATA WITH SDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataframe into`SingleTableMetadata` data type \n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "def create_and_modify_metadata(df):\n",
    "    \"\"\"\n",
    "    SingleTableMetadata type data creation. Obtains information directly from original dataframe and\n",
    "    adjust dtype for \"_id\" type columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        SingleTableMetadata: metadata to create synthetic data\n",
    "    \"\"\"\n",
    "    # Automatically detect metadata from the actual DataFrame\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(df)\n",
    "\n",
    "    \"\"\"# Change dtype of \"_id\" columns. Threat as categorical instead of numerical\n",
    "    for column_name in metadata.columns:\n",
    "        if '_id' in column_name:\n",
    "            metadata.update_column(column_name, sdtype='categorical') \"\"\"\n",
    "    return metadata\n",
    "\n",
    "# create metadata\n",
    "metadata = create_and_modify_metadata(diabetes)\n",
    "\n",
    "# Check if metadata has been correctly generated\n",
    "print(metadata)\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "\n",
    "# Create synthetizer and synthetic data \n",
    "def synthetic_data_creation(md, df):\n",
    "    \"\"\"\n",
    "    Creates synthetizer, trains synthetizer with real data and creates new \n",
    "    synthetic data.\n",
    "    \n",
    "    Parameters:\n",
    "        md (SingleTableMetadata): Metadata of DataFrame.\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        SingleTableMetadata: metadata to create synthetic data\n",
    "    \"\"\"\n",
    "\n",
    "    # create synthetizer\n",
    "    synthesizer = GaussianCopulaSynthesizer(\n",
    "        md,\n",
    "        enforce_min_max_values=True,\n",
    "        enforce_rounding=True) \n",
    "\n",
    "    # train data to learn from real data\n",
    "    synthesizer.fit(\n",
    "        data = df\n",
    "    )\n",
    "\n",
    "    # create new data (same dimensions) based on learned model\n",
    "    synthetic_data = synthesizer.sample(\n",
    "        num_rows=df.shape[0]\n",
    "    )\n",
    "    return synthetic_data\n",
    "\n",
    "# obtain synthetic data\n",
    "synthetic_data = synthetic_data_creation(metadata, diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE SYNTHETIC DATA AND VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions\n",
    "print(f\"Real dimension: {diabetes.shape}\")\n",
    "print(f\"Synth dimension: {synthetic_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information from both datasets\n",
    "real_data_info = pd.DataFrame({\n",
    "    'Column': diabetes.columns,\n",
    "    'Real Non-Null Count':diabetes.notnull().sum()\n",
    "})\n",
    "\n",
    "# For synthetic data\n",
    "synthetic_data_info = pd.DataFrame({\n",
    "    'Column': synthetic_data.columns,\n",
    "    'Synthetic Non-Null Count':synthetic_data.notnull().sum()\n",
    "})\n",
    "\n",
    "# Merge the two DataFrames on the 'Column' name\n",
    "comparison = pd.merge(real_data_info, synthetic_data_info, on='Column', how='outer')\n",
    "\n",
    "# Print comparison table\n",
    "print(\"Comparison of Real and Synthetic Data:\")\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for data correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CORRELATION MATRIX\n",
    "fig, ax = plt.subplots(1,2,figsize = (15,5))\n",
    "corr_r = diabetes.corr()\n",
    "corr_s = synthetic_data.corr()\n",
    "sns.heatmap(corr_r, \n",
    "            xticklabels=corr_r.columns.values,\n",
    "            yticklabels=corr_r.columns.values,\n",
    "            cmap=\"Blues\",\n",
    "            annot=True,         # Display the correlation values in the cells\n",
    "            fmt=\".2f\", ax = ax[0])\n",
    "sns.heatmap(corr_s, \n",
    "            xticklabels=corr_s.columns.values,\n",
    "            yticklabels=corr_s.columns.values,\n",
    "            cmap=\"Greens\",\n",
    "            annot=True,         # Display the correlation values in the cells\n",
    "            fmt=\".2f\", ax = ax[1])\n",
    "ax[0].set_title(\"REAL\")\n",
    "ax[1].set_title(\"SYNTH\")\n",
    "plt.tight_layout()     \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
