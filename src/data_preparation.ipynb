{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "'''# metadata \n",
    "print(diabetes_130_us_hospitals_for_years_1999_2008.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(diabetes_130_us_hospitals_for_years_1999_2008.variables) '''\n",
    "  \n",
    "# fetch dataset \n",
    "diabetes_130_us_hospitals_for_years_1999_2008 = fetch_ucirepo(id=296) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = diabetes_130_us_hospitals_for_years_1999_2008.data.features \n",
    "y = diabetes_130_us_hospitals_for_years_1999_2008.data.targets \n",
    "\n",
    "# create complete real_data\n",
    "diabetes = pd.DataFrame(X)\n",
    "diabetes[\"readmitted\"] = y\n",
    "\n",
    "# visualize data\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE & PREPROCESS REAL DATA\n",
    "\n",
    "### Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions\n",
    "print(f\"Dimension: {diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect sensitive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect sensitive columns by intuition by their name\n",
    "print(f\"\\columns: {diabetes.columns}\\n\")\n",
    "\n",
    "# identify identity sensible data: \n",
    "sensitive_column_names = ['race', 'gender', 'weight', 'admission_type_id','discharge_disposition_id','admission_source_id','payer_code', 'medical_specialty']\n",
    "print(f\"\\nSensitive columns: {sensitive_column_names}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check columns values & distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE COLUMN: check columns values & distribution\n",
    "def visualize_columns_distributions(df):\n",
    "    for col in df.columns:\n",
    "        print(f\"\\n\\nColumn: {col}\")\n",
    "\n",
    "        # Get value counts of data\n",
    "        val_counts = df[col].value_counts(dropna = False)   \n",
    "\n",
    "        # prepare to print more pretty way\n",
    "        counts_df = pd.DataFrame({'Items': val_counts})    \n",
    "\n",
    "        # Print \n",
    "        print(counts_df)\n",
    "    \n",
    "# call to columns_distibution function\n",
    "visualize_columns_distributions(diabetes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generalize 'Nan' race values to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize Nan race as 'Other'\n",
    "import numpy as np\n",
    "diabetes.loc[diabetes[\"race\"].isin([np.nan ,'Other']), \"race\"] = \"Other\"\n",
    "\n",
    "# validate change\n",
    "diabetes.race.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Remove 'Unknown/Invalid' gender values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Unknown/Invalid' # only 3 registry, not possible to define gender, best option would be to remove them \n",
    "diabetes[diabetes['gender'] == 'Unknown/Invalid'] \n",
    "\n",
    "# removing  'Unknown/Invalid' gender data\n",
    "print(f\"Shape before drop: {diabetes.shape}\")\n",
    "diabetes = diabetes.drop(diabetes[diabetes[\"gender\"] == 'Unknown/Invalid'].index)\n",
    "\n",
    "# validating results (only 3 less)\n",
    "print(f\"Shape after drop: {diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check for null values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nulls per columns (percentage)\n",
    "diabetes.isna().sum() * 100 / len(diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Drop \"weight\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove weight column form dataframe 96.858387% null values\n",
    "print(f\"Columns before remove {len(diabetes.columns)}\")\n",
    "diabetes = diabetes.drop('weight', axis=1)\n",
    "print(f\"Columns after remove {len(diabetes.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check for variability\n",
    "\n",
    "Columns that have only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns without variability\n",
    "def columns_without_variability(df):    \n",
    "    \"\"\"\n",
    "    Function that is responsible to determine which columnns has no variability (those which has only 1 value).    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        list(): list of variables without variability.\n",
    "    \"\"\"    \n",
    "    sobran = []\n",
    "\n",
    "    cols = df.columns\n",
    "    for col in cols:\n",
    "        if len(df[col].unique()) < 2:\n",
    "            sobran.append(col)\n",
    "\n",
    "    return sobran\n",
    "\n",
    "# obtener listado de columnas sin variabilidad en una lista\n",
    "cols_without_variability = columns_without_variability(diabetes)\n",
    "\n",
    "# remove columns without variabitly\n",
    "print(f\"Columns without variability: {cols_without_variability}\")\n",
    "print(f\"Columns before remove {len(diabetes.columns)}\")\n",
    "diabetes = diabetes.drop(columns = cols_without_variability)\n",
    "print(f\"Columns after remove {len(diabetes.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for single value entries\n",
    "\n",
    "Columns that have more than one value, but only a single instance for one of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_single_value_entries(df):\n",
    "    rows_to_check = []  # List to hold the rows matching the criteria\n",
    "    cols_to_check = []\n",
    "    for col in df.columns:\n",
    "        # Get the value counts for the column\n",
    "        value_counts = df[col].value_counts()\n",
    "\n",
    "        # Check if exactly one value has a count of 1\n",
    "        if (value_counts == 1).sum() == 1:\n",
    "            # Get the value that appears exactly once\n",
    "            single_value = value_counts[value_counts == 1].index[0]\n",
    "            # add column name\n",
    "            cols_to_check.append(col)\n",
    "            \n",
    "            # Select rows where this single value appears\n",
    "            matching_rows = df[df[col] == single_value]\n",
    "            \n",
    "            # Append these rows to the list\n",
    "            rows_to_check.append(matching_rows)\n",
    "\n",
    "    # Concatenate all the matching rows into a single dataframe (if needed)\n",
    "    result_df = pd.concat(rows_to_check, ignore_index=True) if rows_to_check else pd.DataFrame()\n",
    "\n",
    "    return result_df,cols_to_check\n",
    "\n",
    "# determine single value entries\n",
    "matching_rows_df, cols_to_check = determine_single_value_entries(diabetes)\n",
    "\n",
    "# check data relevancy\n",
    "for col in cols_to_check:\n",
    "    value_counts = diabetes[col].value_counts(dropna = False)\n",
    "    print(f\"Single value entry in column {col} :  {value_counts[value_counts == 1].index[0]}\")\n",
    "    #print(f\"Distribution \\n{value_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct single value entries\n",
    "\n",
    "The columns `metformin-pioglitazone`, `glimepiride-pioglitazone`, and `acetohexamide` show extreme imbalance in their distributions, with only one non-\"No\" entry each. Remove from the original dataframe due to their lack of significant variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter from matching_rows_df those only they have 1 Steady instances\n",
    "steadys = [\"metformin-pioglitazone\",\"glimepiride-pioglitazone\",\"acetohexamide\"] \n",
    "val =\"Steady\"\n",
    "\n",
    "# Filter rows where exactly one of the specified columns has the value 'Steady'\n",
    "filtered_df = diabetes.loc[\n",
    "    (diabetes[steadys[0]] == val).astype(int) +\n",
    "    (diabetes[steadys[1]] == val).astype(int) +\n",
    "    (diabetes[steadys[2]] == val).astype(int) == 1\n",
    "]\n",
    "\n",
    "# Remove rows and columns as they do not have variability after removing\n",
    "print(f\"Actual dimension: {diabetes.shape}\")\n",
    "print(f\"Removing rows: {filtered_df.index}\")\n",
    "diabetes = diabetes.drop(filtered_df.index)\n",
    "print(f\"After dimension: {diabetes.shape}\")\n",
    "print(\"Checking column variability\")\n",
    "cols_without_variability = columns_without_variability(diabetes)\n",
    "print(f\"Removing columns: {cols_without_variability}\")\n",
    "diabetes = diabetes.drop(columns = cols_without_variability)\n",
    "print(f\"Later dimension: {diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check `admission_source_id:13` and `payer_code:FR` cases to determine their actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove single 2 rows \n",
    "print(f\"Actual dimension: {diabetes.shape}\")\n",
    "print(f\"Removing {len(diabetes[(diabetes['admission_source_id'] == 13) ^ (diabetes['payer_code'] == 'FR')])} rows.\")\n",
    "diabetes = diabetes.drop(diabetes[(diabetes[\"admission_source_id\"] == 13) ^ (diabetes[\"payer_code\"] == \"FR\")].index)\n",
    "print(f\"After dimension: {diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check dtype uniformity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "print(f\"\\nData information: {diabetes.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical data comprobations\n",
    "num_cols = diabetes.select_dtypes(include='int64')\n",
    "\n",
    "# Check column values, correspond to dtypes\n",
    "for cat in num_cols.columns:\n",
    "    print(f\"\\nColumn: {cat} values: {diabetes[cat].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change admission_type_id, discharge_disposition_id  &  admission_source_id  to categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# admission_type_id, discharge_disposition_id  &  admission_source_id  are categorical no numericals. Change\n",
    "cols_to_change = [\"admission_type_id\",\"discharge_disposition_id\", \"admission_source_id\"]\n",
    "diabetes[cols_to_change] =  diabetes[cols_to_change].astype(str)\n",
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get updated list\n",
    "num_cols = diabetes.select_dtypes(include='int64')\n",
    "\n",
    "# Check column values, correspond to dtypes\n",
    "for cat in num_cols.columns:\n",
    "    print(f\"\\nColumn: {cat} values: {diabetes[cat].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize memory use changing object to string\n",
    "categorical_cols = diabetes.select_dtypes('object').columns.tolist()\n",
    "\n",
    "# Check column values, correspond to dtypes\n",
    "for cat in categorical_cols:\n",
    "    print(f\"\\nColumn: {cat} values: {diabetes[cat].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check categorical column pair relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# get categorical column pairs\n",
    "categorical_col_pair = list(itertools.combinations(categorical_cols, 2))       \n",
    "\n",
    "# visualize data relations\n",
    "for pair in categorical_col_pair:\n",
    "    print(f\"\\n{pair[0]} distribution per {pair[1]}\")\n",
    "    print(f\"{diabetes.groupby(pair[0])[pair[1]].value_counts(dropna= False).unstack().fillna(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create folder\n",
    "tmp_folder = \"./tmp_folder\"\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "\n",
    "# save data\n",
    "diabetes.to_parquet(os.path.join(tmp_folder,\"refined_file.parquet\"),engine=\"pyarrow\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRECT INBALANCES\n",
    "\n",
    "Examine the data distribution for potential imbalances, which will help determine the appropriate corrective actions.\n",
    "\n",
    "### Visualize data distribution\n",
    "\n",
    "Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# create folder\n",
    "tmp_folder = \"./tmp_folder\"\n",
    "diabetes = pd.read_parquet(os.path.join(tmp_folder,\"generalized_file.parquet\"),engine=\"pyarrow\")\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_cols = diabetes.select_dtypes(include='object')\n",
    "\n",
    "# loop columns\n",
    "for col in categorical_cols:\n",
    "    # plot with col name as title\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))    \n",
    "    sns.countplot(data=diabetes, x=col, ax=ax)\n",
    "    ax.set_title(col)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    max_count = diabetes[col].value_counts().max()\n",
    "    ax.set_ylim(0, max_count * 1.1)  # Adding 10% margin above the max count\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    x_ticks = diabetes[col].value_counts().index\n",
    "    ax.set_xticks(range(len(x_ticks)))\n",
    "    ax.set_xticklabels(x_ticks, rotation=45, ha='right')\n",
    "\n",
    "    # adjust & show\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select continuous columns (int64 type)\n",
    "continuous_columns = diabetes.select_dtypes(include='int64')\n",
    "\n",
    "# Loop through continuous columns\n",
    "for col in continuous_columns:\n",
    "    # Plot histogram\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    sns.histplot(data=diabetes, x=col, ax=ax, bins=30, alpha=0.5)  \n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "    # adjust x-ticks\n",
    "    ax.set_xticks(range(int(diabetes[col].min()), int(diabetes[col].max()) + 1, 2))  \n",
    "    \n",
    "    # Adjust & show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect imbalanced columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_imbalanced_columns(data, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detect columns with imbalanced classes based on the given threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame to check for imbalanced columns.\n",
    "        threshold (float): The proportion threshold to detect imbalance. \n",
    "                           Default is 0.1 (10%). A class is considered imbalanced\n",
    "                           if its proportion is below this threshold or above 1 - threshold.\n",
    "                           \n",
    "    Returns:\n",
    "        imbalanced_cols (list): List of column names with imbalanced data.\n",
    "    \"\"\"\n",
    "    imbalanced_cols = []\n",
    "    \n",
    "    for col in data.columns:\n",
    "        value_counts = data[col].value_counts(normalize=True, dropna=False)     \n",
    "        \n",
    "        # Check if any class proportion is below the threshold or above (1 - threshold)\n",
    "        if any((value_counts < threshold) | (value_counts > (1 - threshold))):\n",
    "            imbalanced_cols.append(col)\n",
    "    \n",
    "    return imbalanced_cols\n",
    "\n",
    "def show_values_proportions(df, col):\n",
    "    \"\"\"\n",
    "    Create dataframe that helps to visualize value_counts and data proportion\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to filter data\n",
    "        col (str): column to filter\n",
    "                           \n",
    "    Returns:\n",
    "        null\n",
    "    \"\"\"\n",
    "     \n",
    "    # Combine proportions into a DataFrame for easy comparison\n",
    "    data =  pd.DataFrame({\n",
    "        'Values': df[col].value_counts(dropna=False),\n",
    "        'Proportions': df[col].value_counts(normalize = True, dropna=False)\n",
    "    }).fillna(0)\n",
    "\n",
    "    # print data\n",
    "    print(data)\n",
    "\n",
    "# recover again data\n",
    "diabetes = pd.read_parquet(os.path.join(tmp_folder,\"refined_file.parquet\"),engine=\"pyarrow\")\n",
    "\n",
    "# find invalances and print results\n",
    "imbalanced_columns = detect_imbalanced_columns(diabetes, threshold= 0.1)\n",
    "if imbalanced_columns:\n",
    "    print(f\"Out of {len(diabetes.columns)} columns, {len(imbalanced_columns)} are imbalanced.\")\n",
    "    for col in imbalanced_columns:\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        # print values\n",
    "        show_values_proportions(diabetes, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply generalization\n",
    "\n",
    "The numerical columns such as `number_emergency`, `num_procedures`, `number_inpatient`, and `number_outpatient` show a large number of instances with no visits, while the rest are spread across their respective values. Generalizing them into 2 groups (whether there have been emergency visits or not) seems like a good action.\n",
    "\n",
    "Similarly, the generalization or grouping of other variables such as `age` and `readmitted` can help better balance the proportions of the data.\n",
    "\n",
    "**Readmitted column**\n",
    "\n",
    "Generalized to `Yes` and `No` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy original data to manipulate\n",
    "df1  = diabetes.copy()\n",
    "\n",
    "# COLUMN: \"readmitted\" \n",
    "col = \"readmitted\"\n",
    "df1.loc[df1[col] != \"NO\", col] = \"Yes\"\n",
    "df1.loc[df1[col] == \"NO\", col] = \"No\"\n",
    "\n",
    "# visualize changes\n",
    "print(\"Real:\")\n",
    "show_values_proportions(diabetes,col)\n",
    "print(\"\\nNew:\")\n",
    "show_values_proportions(df1,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age column**\n",
    "\n",
    "Generalized from 10 groups to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COLUMN: \"age\" \n",
    "GENERALIZE TO\n",
    "[0-20) == [0-10) & [10-20)\n",
    "[20-40) == [20-30) & [30-40)\n",
    "[40-60) == [40-50) & [50-60)\n",
    "[60-80) == [60-70) & [70-80)\n",
    "[80-100) == [80-90) & [90-100)\n",
    "\"\"\"\n",
    "col = \"age\"\n",
    "df1.loc[(df1[col] == \"[0-10)\") | (df1[col] == \"[10-20)\"),col] = \"[0-20)\"\n",
    "df1.loc[(df1[col] == \"[20-30)\") | (df1[col] == \"[30-40)\"),col]= \"[20-40)\"\n",
    "df1.loc[(df1[col] == \"[40-50)\") | (df1[col] == \"[50-60)\"),col] = \"[40-60)\"\n",
    "df1.loc[(df1[col] == \"[60-70)\") | (df1[col] == \"[70-80)\"),col] = \"[60-80)\"\n",
    "df1.loc[(df1[col] == \"[80-90)\") | (df1[col] == \"[90-100)\"),col] = \"[80-100)\"\n",
    "\n",
    "# visualize changes\n",
    "print(\"Real:\")\n",
    "show_values_proportions(diabetes,col)\n",
    "print(\"\\nNew:\")\n",
    "show_values_proportions(df1,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number_outpatient column**\n",
    "\n",
    "Generalized to categorical `Yes` and `No` data. The column name changes to `outpatient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize number_outpatient in two groups\n",
    "col = \"number_outpatient\"\n",
    "col1 = \"outpatient\"\n",
    "df1.loc[df1[col] != 0, col1] = \"Yes\"\n",
    "df1.loc[df1[col] == 0, col1] = \"No\"\n",
    "\n",
    "# visualize changes\n",
    "print(\"Real:\")\n",
    "show_values_proportions(diabetes,col)\n",
    "print(\"\\nNew:\")\n",
    "show_values_proportions(df1,col1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number_impatient column**\n",
    "\n",
    "Generalized to categorical `Yes` and `No` data. The column name changes to `inpatient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize number_impatient in two groups\n",
    "col = \"number_inpatient\"\n",
    "col1 = \"inpatient\"\n",
    "df1.loc[df1[col] != 0, col1] = \"Yes\"\n",
    "df1.loc[df1[col] == 0, col1] = \"No\"\n",
    "\n",
    "# visualize changes\n",
    "print(\"Real:\")\n",
    "show_values_proportions(diabetes,col)\n",
    "print(\"\\nNew:\")\n",
    "show_values_proportions(df1,col1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number_procedures column**\n",
    "\n",
    "Generalized to categorical `Yes` and `No` data. The column name changes to `procedures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize num_procedures in two groups\n",
    "col = \"num_procedures\"\n",
    "col1 = \"procedures\"\n",
    "df1.loc[df1[col] != 0, col1] = \"Yes\"\n",
    "df1.loc[df1[col] == 0, col1] = \"No\"\n",
    "\n",
    "# visualize changes\n",
    "print(\"Real:\")\n",
    "show_values_proportions(diabetes,col)\n",
    "print(\"\\nNew:\")\n",
    "show_values_proportions(df1,col1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number_emergency column**\n",
    "\n",
    "Generalized to categorical `Yes` and `No` data. The column name changes to `emergencies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize number_emergency in two groups\n",
    "col = \"number_emergency\"\n",
    "col1 = \"emergencies\"\n",
    "df1.loc[df1[col] != 0, col1] = \"Yes\"\n",
    "df1.loc[df1[col] == 0, col1] = \"No\"\n",
    "\n",
    "# visualize changes\n",
    "print(\"Real:\")\n",
    "show_values_proportions(diabetes,col)\n",
    "print(\"\\nNew:\")\n",
    "show_values_proportions(df1,col1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original columns \n",
    "col_list = [\"number_emergency\",\"num_procedures\",\"number_inpatient\", \"number_outpatient\"]\n",
    "\n",
    "print(f\"Column length after: {len(df1.columns)} to eliminate: {len(col_list)}\")\n",
    "df1 = df1.drop(col_list, axis=1)\n",
    "print(f\"Column length now: {len(df1.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save generalized file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save generalized data\n",
    "df1.to_parquet(os.path.join(tmp_folder,\"generalized_file.parquet\"),engine=\"pyarrow\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance minority classes\n",
    "\n",
    "Stratify data and oversample minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# set dataframe as reference\n",
    "df = diabetes.copy()\n",
    "col = \"race\"\n",
    "X = df\n",
    "\n",
    "# Contar las instancias de cada clase en la columna 'race'\n",
    "val_counts = X[col].value_counts()\n",
    "\n",
    "# Determinar la cantidad de instancias de la clase mayor (Caucasian)\n",
    "max_count = val_counts.max()\n",
    "\n",
    "# Calcular la cantidad de instancias necesarias para cada clase\n",
    "sampling_strategy = {}\n",
    "for val, count in val_counts.items():\n",
    "    if val in ['Hispanic', 'Asian']:\n",
    "        sampling_strategy[val] = int(max_count * (count / max_count) * 2)  # Duplicar para Hispanic y Asian\n",
    "\n",
    "# Inicializar el oversampler con la nueva estrategia de muestreo\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "\n",
    "# Realizar el sobre-muestreo\n",
    "X_resampled, y_resampled = ros.fit_resample(X, X[col])\n",
    "\n",
    "# comparar ambos resultados\n",
    "comparison_df = pd.DataFrame({\n",
    "        'Real': df[col].value_counts(normalize = True, dropna=False),\n",
    "        \"OverSampled\": X_resampled[col].value_counts(normalize = True, dropna=False)\n",
    "    }).fillna(0)\n",
    "\n",
    "print (f\"Col: {col} \\n{comparison_df}\")\n",
    "\n",
    "df = X_resampled\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding to convert categorical data to numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot function\n",
    "def apply_one_hot(df, cols):\n",
    "    # Apply: one-hot encode categorical variables    \n",
    "    return pd.get_dummies(df, columns=cols, drop_first=True)\n",
    "\n",
    "# LOAD data\n",
    "diabetes = pd.read_parquet(os.path.join(tmp_folder,\"generalized_file.parquet\"),engine=\"pyarrow\")\n",
    "\n",
    "# Set target value\n",
    "X = diabetes.drop(columns='readmitted')\n",
    "y = diabetes['readmitted']\n",
    "\n",
    "# get only categorical data\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# call to function\n",
    "X_encoded = apply_one_hot(X, categorical_cols)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratify data and oversample minority classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# split data in stratified mode\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# apply data balancing to stratified train data\n",
    "smote = SMOTE(sampling_strategy = 'minority', random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Combine back into a DataFrame \n",
    "balanced_train_df = pd.concat([pd.DataFrame(X_train_balanced), pd.Series(y_train_balanced, name='readmitted')], axis=1)\n",
    "balanced_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse one-hot codification to categoricals again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#  TODO: Reverse one-hot encoding\n",
    "categorical_df = pd.DataFrame()\n",
    "\n",
    "for col in X_encoded.columns:\n",
    "    # Get the base name of the original categorical column (before one-hot encoding)\n",
    "    base_name = col.split('_')[0]\n",
    "    categorical_df[base_name] = X_encoded[X_encoded.columns].idxmax(axis=1).str.replace(base_name + '_', '')\n",
    "\n",
    "# If needed, drop duplicates to retain only one instance of the original categorical column\n",
    "categorical_df = categorical_df.drop_duplicates()\n",
    "\n",
    "# Create a new DataFrame with the original categorical columns\n",
    "original_categorical_df = pd.concat([categorical_df, X_encoded.drop(columns=X_encoded.columns)], axis=1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare balanced data with real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#  TODO: comparar ambos resultados\n",
    "for col in diabetes.columns:\n",
    "    comparison_df = pd.DataFrame({\n",
    "            'Real': diabetes[col].value_counts(normalize = True, dropna=False),\n",
    "            \"Balanced\": balanced_train_df[col].value_counts(normalize = True, dropna=False)\n",
    "        }).fillna(0)\n",
    "\n",
    "    print (comparison_df)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE BALANCED DATA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
