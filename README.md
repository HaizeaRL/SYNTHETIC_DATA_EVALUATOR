# Synthetic Data Generation Project

-   **Author**: Haizea Rumayor Lazkano
-   **Last update**: August 2024

------------------------------------------------------------------------

## Overview

This project focuses on the generation of synthetic data, which are artificial copies of real data that maintain the same statistical properties as the original dataset. Synthetic data is becoming increasingly important due to the numerous benefits it offers, particularly in terms of privacy protection and data sharing. By anonymizing data through advanced machine learning techniques, synthetic data generation overcomes many of the limitations of traditional anonymization methods.

### Key Benefits:
- **Enhanced Privacy**: Synthetic data allows for secure sharing of information without compromising sensitive details, significantly improving privacy compared to legacy anonymization techniques.
- **Improved Usability**: Data anonymized through synthetic generation retains a high level of usability, making it easier to explore and democratize the data across teams or organizations.
- **Efficiency**: The automated nature of synthetic data generation enables quick and efficient creation of datasets, reducing time and manual effort.

### Use Cases:
Synthetic data is widely adopted across industries for various purposes, including:
- **Data Privacy and Sharing**: Facilitates the secure sharing of sensitive information while preserving privacy.
- **Product and Service Development**: Synthetic data enables businesses to explore new products and services tailored to different customer segments.
- **Training Machine Learning Models**: Synthetic data can help balance unbalanced datasets, improve model accuracy, and expand small or error-prone datasets.

According to recent industry estimates, by 2024, 60% of the data used for training models will already be synthetic, highlighting the growing importance of this technology.

## Project Objective

In this project, we aim to evaluate the quality and similarity of synthetic data compared to real data. We use the **UCI Machine Learning Repository's Diabetes 130-US Hospitals for Years 1999-2008** dataset and generate synthetic versions through two different methods:
1. **Mostly.AI Platform**: A widely-used tool for synthetic data generation.
2. **Synthetic Data Vault (SDV)**: The most popular open-source synthetic data generator library.

By comparing the results of both techniques, we aim to identify which method produces synthetic data most closely resembling the real dataset, and evaluate their overall effectiveness.

## Tools and Libraries:
- **Mostly.AI**: Commercial synthetic data platform.
- **Synthetic Data Vault (SDV)**: Open-source library for generating and evaluating synthetic data.

The goal is to analyze the quality of synthetic data generated by both platforms and determine which approach is better suited for privacy preservation, data sharing, and machine learning model training.

## Project Structure

The project includes...

## Installation

To ensure a clean and isolated environment for this project, it's essential to create a virtual environment before installing the required Python packages. Once the virtual environment is set up, you should install the required packages.

The necessary packages are listed in the `requirements.txt` file. Install them using the following command:

```bash
pip install -r requirements.txt
```





