# Synthetic Data Generation Project

-   **Author**: Haizea Rumayor Lazkano
-   **Last update**: August 2024

------------------------------------------------------------------------

## Overview

This project focuses on the generation of synthetic data, which are artificial copies of real data that maintain the same statistical properties as the original dataset. Synthetic data is becoming increasingly important due to the numerous benefits it offers, particularly in terms of privacy protection and data sharing. By anonymizing data through advanced machine learning techniques, synthetic data generation overcomes many of the limitations of traditional anonymization methods.

### Key Benefits:
- **Enhanced Privacy**: Synthetic data allows for secure sharing of information without compromising sensitive details, significantly improving privacy compared to legacy anonymization techniques.
- **Improved Usability**: Data anonymized through synthetic generation retains a high level of usability, making it easier to explore and democratize the data across teams or organizations.
- **Efficiency**: The automated nature of synthetic data generation enables quick and efficient creation of datasets, reducing time and manual effort.

### Use Cases:
Synthetic data is widely adopted across industries for various purposes, including:
- **Data Privacy and Sharing**: Facilitates the secure sharing of sensitive information while preserving privacy.
- **Product and Service Development**: Synthetic data enables businesses to explore new products and services tailored to different customer segments.
- **Training Machine Learning Models**: Synthetic data can help balance unbalanced datasets, improve model accuracy, and expand small or error-prone datasets.

According to recent industry estimates, by 2024, 60% of the data used for training models will already be synthetic, highlighting the growing importance of this technology.

## Project Objective

In this project, we aim to evaluate the quality and similarity of synthetic data compared to real data. We use the **UCI Machine Learning Repository's Diabetes 130-US Hospitals for Years 1999-2008** dataset and generate synthetic versions through two different methods:
1. **Mostly.AI Platform**: A widely-used tool for synthetic data generation.
2. **Synthetic Data Vault (SDV)**: The most popular open-source synthetic data generator library.

By comparing the results of both techniques, we aim to identify which method produces synthetic data most closely resembling the real dataset, and evaluate their overall effectiveness.

## Tools and Libraries:
- **Mostly.AI**: Commercial synthetic data platform.
- **Synthetic Data Vault (SDV)**: Open-source library for generating and evaluating synthetic data.

The goal is to analyze the quality of synthetic data generated by both platforms and determine which approach is better suited for privacy preservation, data sharing, and machine learning model training.

## Project Structure

This project is organized to facilitate the creation, evaluation, and comparison of synthetic data. Below is an overview of the project's structure and the purpose of each file:

- **`mostly_ai_synthetic_data_creation.md`**: This file provides detailed instructions for generating synthetic data using the Mostly.AI platform. It includes step-by-step guidance on how to log in, upload data, configure models, and start the training process.

- **`sdv_synthetic_data_creation.ipynb`**: This Jupyter notebook contains instructions for creating synthetic data using the Synthetic Data Vault (SDV). It includes code and explanations for generating and managing synthetic data with SDV tools.

- **`synthetic_data_evaluation_and_comparation.ipynb`**: This Jupyter notebook script evaluates and compares the quality of synthetic data generated by both Mostly.AI and SDV against real data. It provides analysis and insights into which method produces synthetic data that most closely resembles the original dataset.

Each component of the project is designed to ensure a comprehensive approach to synthetic data generation and evaluation.

## Installation

To ensure a clean and isolated environment for this project, it's essential to create a virtual environment before installing the required Python packages. Once the virtual environment is set up, you should install the required packages.

The necessary packages are listed in the `requirements.txt` file. Install them using the following command:

```bash
pip install -r requirements.txt
```





